import scrapy
from pymongo import MongoClient
from urllib.parse import urlparse
from scrapy.spidermiddlewares.httperror import HttpError

class LinkSpider(scrapy.Spider):
    name = "link_spider"

    # Ø¯Ø±ÛŒØ§ÙØª Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù†â€ŒÙ‡Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ Ø§Ø² Ø®Ø· ÙØ±Ù…Ø§Ù†
    def __init__(self, start_url=None, db_name="web_crawler", collection_name="links4", *args, **kwargs):
        super().__init__(*args, **kwargs)

        if start_url:
            self.start_urls = [start_url]
        else:
            self.start_urls = ["https://www.example.com"]

        # Ø§ØªØµØ§Ù„ Ø¨Ù‡ MongoDB Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù†Ø§Ù… Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ùˆ Ú©Ù„Ú©Ø´Ù† Ø¯Ø±ÛŒØ§ÙØªÛŒ
        self.client = MongoClient("mongodb://localhost:27017/")
        self.db = self.client[db_name]
        self.collection = self.db[collection_name]

        # Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ù‚Ø¨Ù„Ø§Ù‹ Ø¯ÛŒØ¯Ù‡â€ŒØ§ÛŒÙ… (Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ø³Ø±Ø¹Øª)
        self.seen_links = set()

        # Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø´Ù…Ø§Ø±Ø´
        self.total_links = 0
        self.duplicate_links = 0
        self.external_links = 0
        self.not_found_links = 0  # Ø´Ù…Ø§Ø±Ø´ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ 404

        # Ø¯Ø§Ù…Ù†Ù‡ Ø§ØµÙ„ÛŒ Ø³Ø§ÛŒØª
        self.main_domain = urlparse(self.start_urls[0]).netloc

    def parse(self, response):
        # Ø§Ú¯Ø± ØµÙØ­Ù‡ 404 Ø¨ÙˆØ¯ØŒ Ø§Ø² Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¢Ù† ØµØ±Ù Ù†Ø¸Ø± Ú©Ù†
        if response.status == 404:
            return

        links = response.css('a::attr(href)').getall()

        for link in links:
            full_url = response.urljoin(link.strip())
            parsed_url = urlparse(full_url)

            # Ø¨Ø±Ø±Ø³ÛŒ Ù„ÛŒÙ†Ú© ØªÚ©Ø±Ø§Ø±ÛŒ Ø¨Ø¯ÙˆÙ† Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³
            if full_url in self.seen_links:
                self.duplicate_links += 1
                continue

            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù„ÛŒÙ†Ú© Ø¨Ù‡ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯ÛŒØ¯Ù‡â€ŒØ´Ø¯Ù‡â€ŒÙ‡Ø§
            self.seen_links.add(full_url)

            # Ø¨Ø±Ø±Ø³ÛŒ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ø±Ø¬ Ø§Ø² Ø¯Ø§Ù…Ù†Ù‡
            if parsed_url.netloc and parsed_url.netloc != self.main_domain:
                self.external_links += 1
                continue

            # Ø°Ø®ÛŒØ±Ù‡ Ù„ÛŒÙ†Ú© Ø¬Ø¯ÛŒØ¯ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³
            self.collection.insert_one({"url": full_url, "status": "pending"})
            self.total_links += 1

            # Ø§Ø±Ø³Ø§Ù„ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ø§ errback Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§Ù‡Ø§ Ù…Ø«Ù„ 404
            yield scrapy.Request(
                full_url,
                callback=self.parse,
                errback=self.handle_error
            )

    def handle_error(self, failure):
        # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø¢ÛŒØ§ Ø®Ø·Ø§ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ú©Ø¯ 404 Ù‡Ø³Øª
        if failure.check(HttpError):
            response = failure.value.response
            if response.status == 404:
                self.logger.info(f"404 error for {response.url}")
                # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ ÙˆØ¶Ø¹ÛŒØª Ù„ÛŒÙ†Ú© Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³
                self.collection.update_one({"url": response.url}, {"$set": {"status": "404"}})
                self.not_found_links += 1

    def closed(self, reason):
        # Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ø¢Ù…Ø§Ø± Scrapy
        stats = self.crawler.stats.get_stats()

        # Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ Ú©Ø±ÙˆÙ„
        report = (
            "\nğŸ“Š **Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ Ú©Ø±ÙˆÙ„:**\n"
            f"ğŸ”¹ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡: {self.total_links}\n"
            f"ğŸ”¸ ØªØ¹Ø¯Ø§Ø¯ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ Ø­Ø°Ùâ€ŒØ´Ø¯Ù‡: {self.duplicate_links}\n"
            f"âš ï¸ ØªØ¹Ø¯Ø§Ø¯ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ø±Ø¬ Ø§Ø² Ø¯Ø§Ù…Ù†Ù‡: {self.external_links}\n"
            f"âŒ ØªØ¹Ø¯Ø§Ø¯ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ 404: {self.not_found_links}\n\n"
            "ğŸ“ˆ **Ø¢Ù…Ø§Ø± Scrapy:**\n"
            f"ğŸ”¹ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§: {stats.get('downloader/request_count', 0)}\n"
            f"ğŸ”¸ ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØªâ€ŒØ´Ø¯Ù‡: {stats.get('downloader/response_count', 0)}\n"
            f"âš ï¸ ØªØ¹Ø¯Ø§Ø¯ Ø®Ø·Ø§Ù‡Ø§ÛŒ Ø¯Ø§Ù†Ù„ÙˆØ¯: {stats.get('downloader/exception_count', 0)}\n"
            f"ğŸ”¹ ØªØ¹Ø¯Ø§Ø¯ Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ø§Ø³Ú©Ø±Ù¾â€ŒØ´Ø¯Ù‡: {stats.get('item_scraped_count', 0)}\n"
            f"ğŸ”¸ Ø­Ø¯Ø§Ú©Ø«Ø± Ø¹Ù…Ù‚ Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§: {stats.get('request_depth_max', 0)}\n"
        )

        print(report)
        # Ø¨Ø³ØªÙ† Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³
        self.client.close()
