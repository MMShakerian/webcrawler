import scrapy
from pymongo import MongoClient
from urllib.parse import urlparse
from scrapy.spidermiddlewares.httperror import HttpError
import os
from datetime import datetime
from scrapy import signals

class LinkSpider(scrapy.Spider):
    name = "link_spider"

    def __init__(self, start_url=None, db_name="web_crawler", collection_name="links4", *args, **kwargs):
        super().__init__(*args, **kwargs)

        if start_url:
            self.start_urls = [start_url]
        else:
            self.start_urls = ["https://www.example.com"]

        # Ø§ØªØµØ§Ù„ Ø¨Ù‡ MongoDB
        self.client = MongoClient("mongodb://localhost:27017/")
        self.db = self.client[db_name]
        self.collection = self.db[collection_name]

        # Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒØ¯Ù‡â€ŒØ´Ø¯Ù‡
        self.seen_links = set()

        # Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø´Ù…Ø§Ø±Ø´
        self.total_links = 0
        self.duplicate_links = 0
        self.external_links = 0
        self.not_found_links = 0

        # Ø¯Ø§Ù…Ù†Ù‡ Ø§ØµÙ„ÛŒ Ø³Ø§ÛŒØª
        self.main_domain = urlparse(self.start_urls[0]).netloc

        # Ù…ØªØºÛŒØ± ØªÙˆÙ‚Ù
        self.paused = False

    @classmethod
    def from_crawler(cls, crawler, *args, **kwargs):
        spider = super(LinkSpider, cls).from_crawler(crawler, *args, **kwargs)
        crawler.signals.connect(spider.pause, signal=signals.spider_idle)
        crawler.signals.connect(spider.resume, signal=signals.spider_opened)
        return spider

    def parse(self, response):
        if self.paused:
            return  # ØªÙˆÙ‚Ù Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§

        if response.status == 404:
            return

        links = response.css('a::attr(href)').getall()
        for link in links:
            if self.paused:
                return  # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø§Ø±Ø³Ø§Ù„ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¬Ø¯ÛŒØ¯

            full_url = response.urljoin(link.strip())
            parsed_url = urlparse(full_url)

            if full_url in self.seen_links:
                self.duplicate_links += 1
                continue

            self.seen_links.add(full_url)

            if parsed_url.netloc and parsed_url.netloc != self.main_domain:
                self.external_links += 1
                continue

            # Ø°Ø®ÛŒØ±Ù‡ Ù„ÛŒÙ†Ú© Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³
            self.collection.insert_one({"url": full_url, "status": "pending"})
            self.total_links += 1

            # Ø§Ø±Ø³Ø§Ù„ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¬Ø¯ÛŒØ¯ ÙÙ‚Ø· Ø¯Ø± ØµÙˆØ±ØªÛŒ Ú©Ù‡ Ú©Ø±Ø§Ù„Ø± Ù…ØªÙˆÙ‚Ù Ù†Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯
            if not self.paused:
                yield scrapy.Request(
                    full_url,
                    callback=self.parse,
                    errback=self.handle_error
                )

    def handle_error(self, failure):
        if failure.check(HttpError):
            response = failure.value.response
            if response.status == 404:
                self.logger.info(f"404 error for {response.url}")
                self.collection.update_one({"url": response.url}, {"$set": {"status": "404"}})
                self.not_found_links += 1

    def pause(self):
        """Ù…ØªÙˆÙ‚Ù Ú©Ø±Ø¯Ù† Ú©Ø±Ø§Ù„Ø±"""
        self.paused = True
        self.logger.info("Crawling paused.")
        self.crawler.engine.pause()

    def resume(self):
        """Ø§Ø¯Ø§Ù…Ù‡ Ø¯Ø§Ø¯Ù† Ú©Ø±Ø§Ù„Ø±"""
        self.paused = False
        self.logger.info("Crawling resumed.")
        self.crawler.engine.unpause()

    def closed(self, reason):
        stats = self.crawler.stats.get_stats()
        report = (
            f"\nğŸ“… **ØªØ§Ø±ÛŒØ® Ø§Ø³ØªØ®Ø±Ø§Ø¬:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
            f"ğŸ”— **Ù„ÛŒÙ†Ú© Ø§Ø³ØªØ®Ø±Ø§Ø¬:** {self.start_urls[0]}\n"
            f"ğŸ—„ **Ù†Ø§Ù… Ø¯ÛŒØªØ§Ø¨ÛŒØ³:** {self.db.name}\n"
            f"ğŸ“‚ **Ù†Ø§Ù… Ú©Ù„Ú©Ø´Ù†:** {self.collection.name}\n\n"
            "ğŸ“Š **Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ Ú©Ø±ÙˆÙ„:**\n"
            f"**ğŸ”¹ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡:** {self.total_links}\n"
            f"**ğŸ”¸ ØªØ¹Ø¯Ø§Ø¯ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ Ø­Ø°Ùâ€ŒØ´Ø¯Ù‡:** {self.duplicate_links}\n"
            f"**âš ï¸ ØªØ¹Ø¯Ø§Ø¯ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ø±Ø¬ Ø§Ø² Ø¯Ø§Ù…Ù†Ù‡:** {self.external_links}\n"
            f"**âŒ ØªØ¹Ø¯Ø§Ø¯ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ 404:** {self.not_found_links}\n\n"
            "ğŸ“ˆ **Ø¢Ù…Ø§Ø± Scrapy:**\n"
            f"**ğŸ”¹ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§:** {stats.get('downloader/request_count', 0)}\n"
            f"**ğŸ”¸ ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØªâ€ŒØ´Ø¯Ù‡:** {stats.get('downloader/response_count', 0)}\n"
            f"**âš ï¸ ØªØ¹Ø¯Ø§Ø¯ Ø®Ø·Ø§Ù‡Ø§ÛŒ Ø¯Ø§Ù†Ù„ÙˆØ¯:** {stats.get('downloader/exception_count', 0)}\n"
            f"**ğŸ”¹ ØªØ¹Ø¯Ø§Ø¯ Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ø§Ø³Ú©Ø±Ù¾â€ŒØ´Ø¯Ù‡:** {stats.get('item_scraped_count', 0)}\n"
            f"**ğŸ”¸ Ø­Ø¯Ø§Ú©Ø«Ø± Ø¹Ù…Ù‚ Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§:** {stats.get('request_depth_max', 0)}\n"
        )

        # Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´
        timestamp = datetime.now().strftime('%Y-%m-%dT%H-%M-%S')
        report_filename = f'report_{timestamp}.txt'
        report_path = os.path.join(os.path.dirname(__file__), '..', 'reports', report_filename)
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)

        latest_report_path = os.path.join(os.path.dirname(__file__), '..', 'reports', 'latest_report.txt')
        with open(latest_report_path, 'w', encoding='utf-8') as f:
            f.write(report)

        print(report)
        self.client.close()
